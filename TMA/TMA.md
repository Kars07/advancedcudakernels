 The primary goal of TMA is to provide an efficient data transfer mechanism from global memory to shared memory for multi-dimensional arrays.
 Naming. Tensor memory accelerator (TMA) is a broad term used to refer to the features described in this section. For the purpose of forward-compatibility and to reduce discrepancies with the PTX ISA, the text in this section refers to TMA operations as either bulk-asynchronous copies or bulk tensor asynchronouscopies, depending on the specific type of copy used. Theterm“bulk”is used to contrast these operations with the asynchronous memory operations described in the previous sections
 Dimensions. TMA supports copying both one-dimensional and multi-dimensional arrays (up to 5 dimensional). The programming model for bulk-asynchronous copies of one-dimensional contiguous arrays is different from the programming model for bulk tensor asynchronous copies of multi dimensional arrays. To perform a bulk tensor asynchronous copy of a multi-dimensional array, thehardware requires a tensor map. This object describes the layout of th multi-dimensional array in global and shared memory. A tensor map is typically created on the host using the cuTensorMapEncode API and then transferred from host to device as a const kernel parameter annotated with __grid_constant__. The tensor map is transferred fromhost todevice as a const kernel parameter annotated with __grid_constant__, and can be used on the device to copy a tile of data between shared and global memory. In contrast, performing a bulk-asynchronous copy of a contiguous one dimensional array does not require a tensor map: it can be performed on-device with a pointer and size parameter.
 Source anddestination. The source and destination addresses of bulk-asynchronous copy operations can bein shared or global memory. The operations can read data from global to shared memory, write data fromsharedtoglobal memory, and also copy from shared memory to Distributed Shared Memory of another block in the same cluster. In addition, when in a cluster, a bulk-asynchronous operation can be specified as being multicast. In this case, data can be transferred from global memory to the shared memory of multiple blocks within the cluster. The multicast feature is optimized for target architecture sm_90a and may have significantly reduced performance on other targets. Hence, it is advised to be used with compute architecture sm_90a.
 Asynchronous. Data transfers using TMA are asynchronous. This allows the initiating thread to continue computing while the hardware asynchronously copies the data. Whether the data transfer occurs asynchronously in practice is up to the hardware implementation and may change in future. There are several completion mechanisms that bulk-asynchronous operations can use to signal that they have completed. When the operation reads from global to shared memory, any thread in the block can wait for the data to be readable in shared memory by waiting on a Shared Memory Barrier. When the bulk-asynchronous operation writes data from shared memory to global or distributed shared memory, only the initiating thread can wait for the operation to have completed. This is accomplished using a bulk async-group based completion mechanism.
